<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:svg="http://www.w3.org/2000/svg" xmlns:x86="http://www.felixcloutier.com/x86"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><link rel="stylesheet" type="text/css" href="style.css"/><a class="dashingAutolink" name="autolink-552"></a><a class="dashAnchor" name="//apple_ref/cpp/Instruction/VMASKMOV"></a><title>VMASKMOV</title></head><body><header><nav><ul><li><a href="index.html">Index</a></li><li>May 2018</li></ul></nav></header><h1>VMASKMOV
		&mdash; Conditional SIMD Packed Loads and Stores</h1>

<table>
<tbody><tr>
<th>Opcode/Instruction</th>
<th>Op/En</th>
<th>64/32-bit Mode</th>
<th>CPUID Feature Flag</th>
<th>Description</th></tr>
<tr>
<td>VEX.NDS.128.66.0F38.W0 2C /r VMASKMOVPS <em>xmm1, xmm2, m128</em></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally load packed single-precision values from <em>m128</em> using mask in <em>xmm2</em> and store in <em>xmm1</em>.</td></tr>
<tr>
<td>VEX.NDS.256.66.0F38.W0 2C /r VMASKMOVPS <em>ymm1, ymm2, m256</em></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally load packed single-precision values from <em>m256</em> using mask in <em>ymm2</em> and store in <em>ymm1</em>.</td></tr>
<tr>
<td>VEX.NDS.128.66.0F38.W0 2D /r VMASKMOVPD <em>xmm1, xmm2, m128</em></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally load packed double-precision values from <em>m128</em> using mask in <em>xmm2</em> and store in <em>xmm1</em>.</td></tr>
<tr>
<td>VEX.NDS.256.66.0F38.W0 2D /r VMASKMOVPD <em>ymm1, ymm2, m256</em></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally load packed double-precision values from <em>m256</em> using mask in <em>ymm2</em> and store in <em>ymm1</em>.</td></tr>
<tr>
<td>VEX.NDS.128.66.0F38.W0 2E /r VMASKMOVPS <em>m128, xmm1, xmm2</em></td>
<td>MVR</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally store packed single-precision values from <em>xmm2</em> using mask in <em>xmm1</em>.</td></tr>
<tr>
<td>VEX.NDS.256.66.0F38.W0 2E /r VMASKMOVPS <em>m256, ymm1, ymm2</em></td>
<td>MVR</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally store packed single-precision values from <em>ymm2</em> using mask in <em>ymm1</em>.</td></tr>
<tr>
<td>VEX.NDS.128.66.0F38.W0 2F /r VMASKMOVPD <em>m128, xmm1, xmm2</em></td>
<td>MVR</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally store packed double-precision values from <em>xmm2</em> using mask in <em>xmm1</em>.</td></tr>
<tr>
<td>VEX.NDS.256.66.0F38.W0 2F /r VMASKMOVPD <em>m256, ymm1, ymm2</em></td>
<td>MVR</td>
<td>V/V</td>
<td>AVX</td>
<td>Conditionally store packed double-precision values from <em>ymm2</em> using mask in <em>ymm1</em>.</td></tr></tbody></table>
<h2 id="instruction-operand-encoding">Instruction Operand Encoding<a class="anchor" href="VMASKMOV.html#instruction-operand-encoding">
			&para;
		</a></h2>
<table>
<tbody><tr>
<td>Op/En</td>
<td>Operand 1</td>
<td>Operand 2</td>
<td>Operand 3</td>
<td>Operand 4</td></tr>
<tr>
<td>RVM</td>
<td>ModRM:reg (w)</td>
<td>VEX.vvvv (r)</td>
<td>ModRM:r/m (r)</td>
<td>NA</td></tr>
<tr>
<td>MVR</td>
<td>ModRM:r/m (w)</td>
<td>VEX.vvvv (r)</td>
<td>ModRM:reg (r)</td>
<td>NA</td></tr></tbody></table>
<h2 id="description">Description<a class="anchor" href="VMASKMOV.html#description">
			&para;
		</a></h2>
<p>Conditionally moves packed data elements from the second source operand into the corresponding data element of the destination operand, depending on the mask bits associated with each data element. The mask bits are specified in the first source operand.</p>
<p>The mask bit for each data element is the most significant bit of that element in the first source operand. If a mask is 1, the corresponding data element is copied from the second source operand to the destination operand. If the mask is 0, the corresponding data element is set to zero in the load form of these instructions, and unmodified in the store form.</p>
<p>The second source operand is a memory address for the load form of these instruction. The destination operand is a memory address for the store form of these instructions. The other operands are both XMM registers (for VEX.128 version) or YMM registers (for VEX.256 version).</p>
<p>Faults occur only due to mask-bit required memory accesses that caused the faults. Faults will not occur due to referencing any memory location if the corresponding mask bit for that memory location is 0. For example, no faults will be detected if the mask bits are all zero.</p>
<p>Unlike previous MASKMOV instructions (MASKMOVQ and MASKMOVDQU), a nontemporal hint is not applied to these instructions.</p>
<p>Instruction behavior on alignment check reporting with mask bits of less than all 1s are the same as with mask bits of all 1s.</p>
<p>VMASKMOV should not be used to access memory mapped I/O and un-cached memory as the access and the ordering of the individual loads or stores it does is implementation specific.</p>
<p>In cases where mask bits indicate data should not be loaded or stored paging A and D bits will be set in an implementation dependent way. However, A and D bits are always set for pages where data is actually loaded/stored.</p>
<p>Note: for load forms, the first source (the mask) is encoded in VEX.vvvv; the second source is encoded in rm_field, and the destination register is encoded in reg_field.</p>
<p>Note: for store forms, the first source (the mask) is encoded in VEX.vvvv; the second source register is encoded in reg_field, and the destination memory location is encoded in rm_field.</p>
<h2 id="operation">Operation<a class="anchor" href="VMASKMOV.html#operation">
			&para;
		</a></h2>
<h3 id="vmaskmovps--128-bit-load">VMASKMOVPS -128-bit load<a class="anchor" href="VMASKMOV.html#vmaskmovps--128-bit-load">
			&para;
		</a></h3>
<pre>DEST[31:0]&larr;IF (SRC1[31]) Load_32(mem) ELSE 0
DEST[63:32]&larr;IF (SRC1[63]) Load_32(mem + 4) ELSE 0
DEST[95:64]&larr;IF (SRC1[95]) Load_32(mem + 8) ELSE 0
DEST[127:97]&larr;IF (SRC1[127]) Load_32(mem + 12) ELSE 0
DEST[MAXVL-1:128] &larr; 0
</pre>
<h3 id="vmaskmovps---256-bit-load">VMASKMOVPS - 256-bit load<a class="anchor" href="VMASKMOV.html#vmaskmovps---256-bit-load">
			&para;
		</a></h3>
<pre>DEST[31:0]&larr;IF (SRC1[31]) Load_32(mem) ELSE 0
DEST[63:32]&larr;IF (SRC1[63]) Load_32(mem + 4) ELSE 0
DEST[95:64]&larr;IF (SRC1[95]) Load_32(mem + 8) ELSE 0
DEST[127:96]&larr;IF (SRC1[127]) Load_32(mem + 12) ELSE 0
DEST[159:128]&larr;IF (SRC1[159]) Load_32(mem + 16) ELSE 0
DEST[191:160]&larr;IF (SRC1[191]) Load_32(mem + 20) ELSE 0
DEST[223:192]&larr;IF (SRC1[223]) Load_32(mem + 24) ELSE 0
DEST[255:224]&larr;IF (SRC1[255]) Load_32(mem + 28) ELSE 0
</pre>
<h3 id="vmaskmovpd---128-bit-load">VMASKMOVPD - 128-bit load<a class="anchor" href="VMASKMOV.html#vmaskmovpd---128-bit-load">
			&para;
		</a></h3>
<pre>DEST[63:0]&larr;IF (SRC1[63]) Load_64(mem) ELSE 0
DEST[127:64]&larr;IF (SRC1[127]) Load_64(mem + 16) ELSE 0
DEST[MAXVL-1:128] &larr; 0
</pre>
<h3 id="vmaskmovpd---256-bit-load">VMASKMOVPD - 256-bit load<a class="anchor" href="VMASKMOV.html#vmaskmovpd---256-bit-load">
			&para;
		</a></h3>
<pre>DEST[63:0]&larr;IF (SRC1[63]) Load_64(mem) ELSE 0
DEST[127:64]&larr;IF (SRC1[127]) Load_64(mem + 8) ELSE 0
DEST[195:128]&larr;IF (SRC1[191]) Load_64(mem + 16) ELSE 0
DEST[255:196]&larr;IF (SRC1[255]) Load_64(mem + 24) ELSE 0
</pre>
<h3 id="vmaskmovps---128-bit-store">VMASKMOVPS - 128-bit store<a class="anchor" href="VMASKMOV.html#vmaskmovps---128-bit-store">
			&para;
		</a></h3>
<pre>IF (SRC1[31]) DEST[31:0]&larr;SRC2[31:0]
IF (SRC1[63]) DEST[63:32]&larr;SRC2[63:32]
IF (SRC1[95]) DEST[95:64]&larr;SRC2[95:64]
IF (SRC1[127]) DEST[127:96]&larr;SRC2[127:96]
</pre>
<h3 id="vmaskmovps---256-bit-store">VMASKMOVPS - 256-bit store<a class="anchor" href="VMASKMOV.html#vmaskmovps---256-bit-store">
			&para;
		</a></h3>
<pre>IF (SRC1[31]) DEST[31:0]&larr;SRC2[31:0]
IF (SRC1[63]) DEST[63:32]&larr;SRC2[63:32]
IF (SRC1[95]) DEST[95:64]&larr;SRC2[95:64]
IF (SRC1[127]) DEST[127:96]&larr;SRC2[127:96]
IF (SRC1[159]) DEST[159:128]&larr;SRC2[159:128]
IF (SRC1[191]) DEST[191:160]&larr;SRC2[191:160]
IF (SRC1[223]) DEST[223:192]&larr;SRC2[223:192]
IF (SRC1[255]) DEST[255:224]&larr;SRC2[255:224]
</pre>
<h3 id="vmaskmovpd---128-bit-store">VMASKMOVPD - 128-bit store<a class="anchor" href="VMASKMOV.html#vmaskmovpd---128-bit-store">
			&para;
		</a></h3>
<pre>IF (SRC1[63]) DEST[63:0]&larr;SRC2[63:0]
IF (SRC1[127]) DEST[127:64]&larr;SRC2[127:64]
</pre>
<h3 id="vmaskmovpd---256-bit-store">VMASKMOVPD - 256-bit store<a class="anchor" href="VMASKMOV.html#vmaskmovpd---256-bit-store">
			&para;
		</a></h3>
<pre>IF (SRC1[63]) DEST[63:0]&larr;SRC2[63:0]
IF (SRC1[127]) DEST[127:64]&larr;SRC2[127:64]
IF (SRC1[191]) DEST[191:128]&larr;SRC2[191:128]
IF (SRC1[255]) DEST[255:192]&larr;SRC2[255:192]
</pre>
<h2 id="intel-c-c++-compiler-intrinsic-equivalent">Intel C/C++ Compiler Intrinsic Equivalent<a class="anchor" href="VMASKMOV.html#intel-c-c++-compiler-intrinsic-equivalent">
			&para;
		</a></h2>
<pre>__m256 _mm256_maskload_ps(float const *a, __m256i mask)
</pre>
<pre>void _mm256_maskstore_ps(float *a, __m256i mask, __m256 b)
</pre>
<pre>__m256d _mm256_maskload_pd(double *a, __m256i mask);
</pre>
<pre>void _mm256_maskstore_pd(double *a, __m256i mask, __m256d b);
</pre>
<pre>__m128 _mm_maskload_ps(float const *a, __m128i mask)
</pre>
<pre>void _mm_maskstore_ps(float *a, __m128i mask, __m128 b)
</pre>
<pre>__m128d _mm_maskload_pd(double const *a, __m128i mask);
</pre>
<pre>void _mm_maskstore_pd(double *a, __m128i mask, __m128d b);
</pre>
<h2 class="exceptions" id="simd-floating-point-exceptions">SIMD Floating-Point Exceptions<a class="anchor" href="VMASKMOV.html#simd-floating-point-exceptions">
			&para;
		</a></h2>
<p>None</p>
<h2 class="exceptions" id="other-exceptions">Other Exceptions<a class="anchor" href="VMASKMOV.html#other-exceptions">
			&para;
		</a></h2>
<p>See Exceptions Type 6 (No AC# reported for any mask bit combinations);</p>
<p>additionally</p>
<table>
<tbody><tr>
<td>#UD</td>
<td>If VEX.W = 1.</td></tr></tbody></table><footer><p>
		This UNOFFICIAL, mechanically-separated, non-verified reference is provided for convenience, but it may be
		inc<span style="opacity: 0.2">omp</span>lete or b<sub>r</sub>oke<sub>n</sub> in various obvious or non-obvious
		ways. Refer to <a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">Intel&reg; 64 and IA-32 Architectures Software Developer&rsquo;s Manual</a> for anything serious.
	</p></footer>
</body></html>